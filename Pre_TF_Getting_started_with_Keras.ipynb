{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tqsyed/fc2conv-tf/blob/main/Pre_TF_Getting_started_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHxW8RkRcmwg"
      },
      "source": [
        "Run the cell below if using Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQPC4bYXcb9e"
      },
      "source": [
        "# %tensorflow_version 2.x "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT_Vm27Obn6i"
      },
      "source": [
        "# What is KERAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYUnEg5tbn6j"
      },
      "source": [
        "You have just found Keras.\n",
        "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
        "\n",
        "Use Keras if you need a deep learning library that:\n",
        "\n",
        "    Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
        "    Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
        "    Runs seamlessly on CPU and GPU.\n",
        "Read the documentation at  https://keras.io/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TZVRQvlbn6k"
      },
      "source": [
        "# Types of keras models\n",
        "### Sequential  Models\n",
        "The Sequential model is a linear stack of layers.\n",
        "### Functional API\n",
        "The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FgLyiPgbn6l"
      },
      "source": [
        "## Creating dummy dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYo1wxTMbn6m"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.datasets as datasets\n",
        "import tensorflow as tf\n",
        "from  tensorflow.python import keras\n",
        "import numpy as np\n",
        "\n",
        "X, y = datasets.make_blobs(n_samples = 10000, n_features=100, centers=10)\n",
        "y = tf.keras.utils.to_categorical(y, 10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDj-uXHtbn6r"
      },
      "source": [
        "## Sequential Models on dummy dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYduFtgObn6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "768eadd9-5b07-41be-8d39-2bec06c87a22"
      },
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import Sequential\n",
        "\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
        "model.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "210/210 [==============================] - 1s 3ms/step - loss: 0.0530 - accuracy: 0.9925\n",
            "Epoch 2/10\n",
            "210/210 [==============================] - 1s 3ms/step - loss: 6.7024e-04 - accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "210/210 [==============================] - 1s 4ms/step - loss: 4.0207e-04 - accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "210/210 [==============================] - 1s 4ms/step - loss: 2.8923e-04 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "210/210 [==============================] - 1s 3ms/step - loss: 2.2650e-04 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "210/210 [==============================] - 1s 3ms/step - loss: 1.8653e-04 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "210/210 [==============================] - 1s 4ms/step - loss: 1.5871e-04 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "210/210 [==============================] - 1s 3ms/step - loss: 1.3828e-04 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "210/210 [==============================] - 1s 3ms/step - loss: 1.2260e-04 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "210/210 [==============================] - 1s 3ms/step - loss: 1.1017e-04 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1d802956d0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T54id0Qbn6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d88e3cf4-f5be-4684-f202-7c1d5b74d3c8"
      },
      "source": [
        "loss_and_metrics = model.evaluate(X_test, y_test, batch_size=128)\n",
        "print('Test loss:', loss_and_metrics[0])\n",
        "print('Test accuracy:', loss_and_metrics[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0760e-04 - accuracy: 1.0000\n",
            "Test loss: 0.00010759710130514577\n",
            "Test accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYKrPjA582HD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d744b8-cd43-4782-ea28-22bb7ef0f62d"
      },
      "source": [
        "model.predict(X_test).round()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovmp2qNTbn69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fab13f0-d9bd-4b1e-bed4-45cae3b798aa"
      },
      "source": [
        "print(y_test[0])\n",
        "print(np.argmax(y_test[0]))\n",
        "prediction = model.predict(X_test[0:1])\n",
        "print(prediction)\n",
        "print(np.argmax(prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "8\n",
            "[[2.1581754e-06 3.4043394e-06 7.6118986e-06 4.1452995e-06 1.1459590e-06\n",
            "  3.1871497e-10 3.7967482e-07 2.9832097e-09 9.9998105e-01 9.7250577e-08]]\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2srmQJZbn7B"
      },
      "source": [
        "## Sequential Models on the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-Bvm5lAbn7D"
      },
      "source": [
        "import keras\n",
        "from tensorflow.python.keras.datasets import mnist\n",
        "from tensorflow.python.keras import Sequential\n",
        "#from keras.datasets import mnist\n",
        "#from keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB7AVvBCbn7G"
      },
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3-4D0UNbn7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daddf370-4628-43da-d8dd-833bf48994ce"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMyZdTAZ4EX8"
      },
      "source": [
        "### Flatten the images into single vector\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-krBGPdLbn7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe7c5f4-9c74-4bc7-8372-a18cf2f6b368"
      },
      "source": [
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ljqqX0ybn7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04f589d-b5d9-45dd-b70b-f5a42b570ff9"
      },
      "source": [
        "y_test_copy = y_test.copy()\n",
        "\n",
        "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "# y_test = keras.utils.to_categorical(y_test, num_classes) \n",
        "from keras.utils import np_utils\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 10)\n",
            "(10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKAN-bogbn7S"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_dim=784))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T8Bhi_Jbn7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "500a7e39-0d57-48e0-87ab-b648c765204f"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 4.1390 - accuracy: 0.8907\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3821 - accuracy: 0.9420\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.2729 - accuracy: 0.9541\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.2461 - accuracy: 0.9619\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.2171 - accuracy: 0.9646\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1943 - accuracy: 0.9692\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1880 - accuracy: 0.9730\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1836 - accuracy: 0.9748\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1674 - accuracy: 0.9771\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1588 - accuracy: 0.9781\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1517 - accuracy: 0.9799\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1535 - accuracy: 0.9811\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1559 - accuracy: 0.9822\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1200 - accuracy: 0.9849\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1448 - accuracy: 0.9844\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1367 - accuracy: 0.9854\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1215 - accuracy: 0.9872\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1210 - accuracy: 0.9890\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1288 - accuracy: 0.9885\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1166 - accuracy: 0.9894\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd4e4146a10>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYquiFEibn7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0de686c-dafc-4081-bb00-993e3b7b30d7"
      },
      "source": [
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print('Test loss:', loss_and_metrics[0])\n",
        "print('Test accuracy:', loss_and_metrics[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.9710\n",
            "Test loss: 0.6406213641166687\n",
            "Test accuracy: 0.9710000157356262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lc8EvxUbn7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feeb5510-1dce-41e4-81dd-0833848516b6"
      },
      "source": [
        "example = 100\n",
        "print(y_test[example])\n",
        "print(np.argmax(y_test[example]))\n",
        "prediction = model.predict(x_test[example:example+1])\n",
        "print(prediction)\n",
        "print(np.argmax(prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "6\n",
            "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8dEnDiRbn7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c41292-a13b-48b3-aed9-823cdefb0124"
      },
      "source": [
        "print(y_test_copy[example])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6yg4XFhbn7j"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkBmH69Hbn7m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "58d94df1-7d38-4056-956a-9c019f4c27ad"
      },
      "source": [
        "plt.imshow(x_train[example], cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd5500db950>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QoYZjGJ53SZ"
      },
      "source": [
        " ## Why such a low accuracy? Let's have a look at the input.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTJoZUa852SP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b878e4-5779-46bc-d59c-3fb0aeb78818"
      },
      "source": [
        "print(x_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
            "  175  26 166 255 247 127   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
            "  225 172 253 242 195  64   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
            "   93  82  82  56  39   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
            "   25   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
            "  150  27   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
            "  253 187   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
            "  253 249  64   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
            "  253 207   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
            "  250 182   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
            "   78   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do1WShssbn7q"
      },
      "source": [
        "## Feeding in normalized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5_fkAlNbn7q"
      },
      "source": [
        "import keras\n",
        "from tensorflow.python.keras.datasets import mnist\n",
        "from tensorflow.python.keras import Sequential\n",
        "#from keras.datasets import mnist\n",
        "#from keras.models import Sequential\n",
        "\n",
        "\n",
        "\n",
        "#import keras\n",
        "#from keras.datasets import mnist\n",
        "#from keras.models import Sequential\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 20\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255                                # <<<<< notice this\n",
        "x_test /= 255                                 # <<<<< notice this\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "from keras.utils import np_utils\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_dim=784))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print('Test loss:', loss_and_metrics[0])\n",
        "print('Test accuracy:', loss_and_metrics[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GKELIv-bn7t"
      },
      "source": [
        "## Run this multiple times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4W1LdFKbn7u"
      },
      "source": [
        "import keras\n",
        "from tensorflow.python.keras.datasets import mnist\n",
        "from tensorflow.python.keras import Sequential\n",
        "#from keras.datasets import mnist\n",
        "#from keras.models import Sequential\n",
        "\n",
        "#import keras\n",
        "#from keras.datasets import mnist\n",
        "#from keras.models import Sequential\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 5\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "from keras.utils import np_utils\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_dim=784))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print('Test loss:', loss_and_metrics[0])\n",
        "print('Test accuracy:', loss_and_metrics[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXkl1E4Abn7x"
      },
      "source": [
        "## Initializers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_ooqtWhbn7y"
      },
      "source": [
        "import keras\n",
        "from tensorflow.python.keras.datasets import mnist\n",
        "from tensorflow.python.keras import Sequential\n",
        "#from keras.datasets import mnist\n",
        "#from keras.models import Sequential\n",
        "\n",
        "#import keras\n",
        "#from keras.datasets import mnist\n",
        "#from keras.models import Sequential\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 5\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "from keras.utils import np_utils\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, kernel_initializer='lecun_uniform', activation='relu', input_dim=784))\n",
        "model.add(Dense(512, kernel_initializer='lecun_uniform', activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print('Test loss:', loss_and_metrics[0])\n",
        "print('Test accuracy:', loss_and_metrics[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wadGt6rXbn73"
      },
      "source": [
        "## Batch normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ra1DzS_bn74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e3701a0-3671-4567-ae55-e7986919975c"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "#from tensorflow.python.keras.datasets import mnist\n",
        "#from tensorflow.python.keras import Sequential\n",
        "#import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 5\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(Dense(512, activation='relu', input_dim=784))\n",
        "model.add(keras.layers.BatchNormalization())                   ### <<<<< notice this\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print('Test loss:', loss_and_metrics[0])\n",
        "print('Test accuracy:', loss_and_metrics[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 784)\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.4620 - accuracy: 0.8693\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.2166 - accuracy: 0.9388\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1685 - accuracy: 0.9519\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1398 - accuracy: 0.9602\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1207 - accuracy: 0.9653\n",
            "79/79 [==============================] - 0s 4ms/step - loss: 0.1216 - accuracy: 0.9648\n",
            "Test loss: 0.12162065505981445\n",
            "Test accuracy: 0.9648000001907349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZDp68YVbn79"
      },
      "source": [
        "### Combining things "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOsnCfHGbn7-"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "#from tensorflow.python.keras.datasets import mnist\n",
        "#from tensorflow.python.keras import Sequential\n",
        "#import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "\n",
        "\n",
        "#import keras\n",
        "#from keras.datasets import mnist\n",
        "#from keras.models import Sequential\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 5\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255                             \n",
        "x_test /= 255  \n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "from keras.utils import np_utils\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(Dense(512, kernel_initializer='he_normal', activation='relu', input_dim=784))\n",
        "model.add(keras.layers.BatchNormalization())                   ### <<<<< notice this\n",
        "model.add(Dense(512, kernel_initializer='he_normal', activation='relu'))\n",
        "model.add(Dense(512, kernel_initializer='he_normal', activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print('Test loss:', loss_and_metrics[0])\n",
        "print('Test accuracy:', loss_and_metrics[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fnuLFeXnwOm"
      },
      "source": [
        "##The 2nd dataset \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbPHc60ibn8B"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "#from tensorflow.python.keras.datasets import mnist\n",
        "#from tensorflow.python.keras import Sequential\n",
        "#import keras\n",
        "#from keras.datasets import mnist\n",
        "#from keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras import Sequential\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 5\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print(x_train.shape)\n",
        "x_train = x_train.reshape(50000, 3072)\n",
        "x_test = x_test.reshape(10000, 3072)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255                             \n",
        "x_test /= 255  \n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "from keras.utils import np_utils\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(Dense(512,  activation='relu', input_dim=3072))             ### <<<<< notice this\n",
        "model.add(Dense(512,  activation='relu'))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print('Test loss:', loss_and_metrics[0])\n",
        "print('Test accuracy:', loss_and_metrics[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHk5FRYy9tDZ"
      },
      "source": [
        "## Let's Play with a CNN now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1OKjsiZULtw"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MBSSgtG1_p_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae3ba1b-b1db-42f1-b83c-caf7dcf2bd0f"
      },
      "source": [
        "import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.models import load_model \n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "# from keras import optimizers\n",
        "import numpy as np\n",
        " \n",
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    elif epoch > 100:\n",
        "        lrate = 0.0003       \n",
        "    return lrate\n",
        " \n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        " \n",
        "#z-score\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)\n",
        " \n",
        "num_classes = 10\n",
        "y_train = np_utils.to_categorical(y_train,num_classes)\n",
        "y_test = np_utils.to_categorical(y_test,num_classes)\n",
        " \n",
        "weight_decay = 1e-4\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        " \n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        " \n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        " \n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        " \n",
        "model.summary()\n",
        " \n",
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "datagen.fit(x_train)\n",
        " \n",
        "#training\n",
        "batch_size = 64\n",
        "\n",
        "opt_rms = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=2,\\\n",
        "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
        "\n",
        "\n",
        "#save to disk\n",
        "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "del model  # deletes the existing model\n",
        "\n",
        "# returns a compiled model\n",
        "# identical to the previous one\n",
        "model = load_model('my_model.h5')\n",
        "  \n",
        "#testing\n",
        "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
        "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 32, 32, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:86: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "781/781 [==============================] - 58s 64ms/step - loss: 2.0222 - accuracy: 0.3742 - val_loss: 1.4973 - val_accuracy: 0.4971 - lr: 0.0010\n",
            "Epoch 2/2\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 1.5090 - accuracy: 0.4869 - val_loss: 1.3916 - val_accuracy: 0.5456 - lr: 0.0010\n",
            "79/79 [==============================] - 2s 16ms/step - loss: 1.3916 - accuracy: 0.5456\n",
            "\n",
            "Test result: 54.560 loss: 1.392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c30Jj34wVwgF"
      },
      "source": [
        "\n",
        "# from keras import optimizers \n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='mean_squared_error', optimizer=sgd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P7YvSYI2AiS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1dd5bbec-2efe-414a-8e92-9b4ee7b2c405"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        " \n",
        "#z-score\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)\n",
        " \n",
        "num_classes = 10\n",
        "y_train = np_utils.to_categorical(y_train,num_classes)\n",
        "y_test = np_utils.to_categorical(y_test,num_classes)\n",
        "\n",
        "\n",
        "weight_decay = 1e-4\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        " \n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        " \n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        " \n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        " \n",
        "model.summary()\n",
        " \n",
        "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
        " \n",
        "model.fit(x_train, y_train, epochs=2, batch_size=32, verbose=1,validation_data=(x_test,y_test))\n",
        "\n",
        "#testing\n",
        "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
        "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 113,738\n",
            "Trainable params: 113,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "50000/50000 [==============================] - 14s 279us/step - loss: 1.3970 - acc: 0.5139 - val_loss: 1.0983 - val_acc: 0.6289\n",
            "Epoch 2/2\n",
            "50000/50000 [==============================] - 9s 172us/step - loss: 1.1359 - acc: 0.6151 - val_loss: 0.9901 - val_acc: 0.6683\n",
            "10000/10000 [==============================] - 0s 33us/step\n",
            "\n",
            "Test result: 66.830 loss: 0.990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi1GobkFl31c"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}