{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tqsyed/fc2conv-tf/blob/main/TF_Keras_springboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHxW8RkRcmwg"
      },
      "source": [
        "Run the cell below if using Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQPC4bYXcb9e"
      },
      "source": [
        "# %tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT_Vm27Obn6i"
      },
      "source": [
        "# What is KERAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYUnEg5tbn6j"
      },
      "source": [
        "You have just found Keras.\n",
        "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
        "\n",
        "Use Keras if you need a deep learning library that:\n",
        "\n",
        "    Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
        "    Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
        "    Runs seamlessly on CPU and GPU.\n",
        "Read the documentation at  https://keras.io/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TZVRQvlbn6k"
      },
      "source": [
        "# Types of keras models\n",
        "### Sequential  Models\n",
        "The Sequential model is a linear stack of layers.\n",
        "### Functional API\n",
        "The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FgLyiPgbn6l"
      },
      "source": [
        "## Creating dummy dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYo1wxTMbn6m"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.datasets as datasets\n",
        "import tensorflow as tf\n",
        "from  tensorflow.python import keras\n",
        "import numpy as np\n",
        "\n",
        "X, y = datasets.make_blobs(n_samples = 10000, n_features=100, centers=10)\n",
        "y = tf.keras.utils.to_categorical(y, 10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDj-uXHtbn6r"
      },
      "source": [
        "## Sequential Models on dummy dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYduFtgObn6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "312858b1-f9a0-4bd3-a434-586b9d8ad73b"
      },
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import Sequential\n",
        "\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
        "model.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "210/210 [==============================] - 5s 3ms/step - loss: 0.1026 - accuracy: 0.9881\n",
            "Epoch 2/10\n",
            "210/210 [==============================] - 1s 3ms/step - loss: 9.3429e-04 - accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "210/210 [==============================] - 1s 3ms/step - loss: 5.4979e-04 - accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "210/210 [==============================] - 1s 3ms/step - loss: 3.9190e-04 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 3.0499e-04 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 2.4982e-04 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.1169e-04 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "210/210 [==============================] - 1s 4ms/step - loss: 1.8368e-04 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "210/210 [==============================] - 1s 3ms/step - loss: 1.6222e-04 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "210/210 [==============================] - 1s 3ms/step - loss: 1.4517e-04 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d30298455d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T54id0Qbn6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a897955-0a92-439a-dc50-bba6ada85560"
      },
      "source": [
        "loss_and_metrics = model.evaluate(X_test, y_test, batch_size=128)\n",
        "print('Test loss:', loss_and_metrics[0])\n",
        "print('Test accuracy:', loss_and_metrics[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26/26 [==============================] - 0s 2ms/step - loss: 1.3408e-04 - accuracy: 1.0000\n",
            "Test loss: 0.00013408400991465896\n",
            "Test accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYKrPjA582HD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1f4048-f778-49fd-c93b-caf56e20774a"
      },
      "source": [
        "model.predict(X_test).round()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "104/104 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovmp2qNTbn69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbefbcf5-a975-46b7-c8c3-364a6df8946a"
      },
      "source": [
        "print(y_test[0])\n",
        "print(np.argmax(y_test[0]))\n",
        "prediction = model.predict(X_test[0:1])\n",
        "print(prediction)\n",
        "print(np.argmax(prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "9\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "[[1.3872216e-04 1.2335268e-07 1.8237154e-06 1.6328966e-06 9.8086539e-06\n",
            "  2.8697825e-10 1.0311542e-06 5.2409021e-05 4.0085997e-06 9.9979037e-01]]\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2srmQJZbn7B"
      },
      "source": [
        "## Sequential Models on the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-Bvm5lAbn7D"
      },
      "source": [
        "import keras\n",
        "# from tensorflow.python.keras.datasets import mnist\n",
        "# from tensorflow.python.keras import Sequential\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB7AVvBCbn7G"
      },
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3-4D0UNbn7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "764e9f67-6ee3-4af3-cf9b-cb9829ecfed0"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMyZdTAZ4EX8"
      },
      "source": [
        "### Flatten the images into single vector\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-krBGPdLbn7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf81559e-2bdf-49b6-8112-09b27389960f"
      },
      "source": [
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ljqqX0ybn7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2952cfe-d7e8-4b28-f009-7d4eea9a9f16"
      },
      "source": [
        "y_test_copy = y_test.copy()\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "# from keras.utils import np_utils\n",
        "# y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "# y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 10)\n",
            "(10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKAN-bogbn7S"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_dim=784))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T8Bhi_Jbn7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3886aab1-9b86-45e1-a9c6-179bded22edb",
        "collapsed": true
      },
      "source": [
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 3s 3ms/step - loss: 3.3563 - accuracy: 0.8913\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3521 - accuracy: 0.9421\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2730 - accuracy: 0.9527\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2229 - accuracy: 0.9605\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.1981 - accuracy: 0.9668\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.1876 - accuracy: 0.9712\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.1628 - accuracy: 0.9748\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.1561 - accuracy: 0.9766\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.1467 - accuracy: 0.9785\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.1280 - accuracy: 0.9807\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1285 - accuracy: 0.9825\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1392 - accuracy: 0.9828\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.1235 - accuracy: 0.9842\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.1209 - accuracy: 0.9847\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.1241 - accuracy: 0.9866\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.1034 - accuracy: 0.9877\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0977 - accuracy: 0.9879\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.1066 - accuracy: 0.9892\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1029 - accuracy: 0.9900\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.1100 - accuracy: 0.9898\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d300c233f70>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYquiFEibn7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0de686c-dafc-4081-bb00-993e3b7b30d7"
      },
      "source": [
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print('Test loss:', loss_and_metrics[0])\n",
        "print('Test accuracy:', loss_and_metrics[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.9710\n",
            "Test loss: 0.6406213641166687\n",
            "Test accuracy: 0.9710000157356262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lc8EvxUbn7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "033e420c-6df0-45f0-ebb1-60107b1a771f"
      },
      "source": [
        "example = 100\n",
        "print(y_test[example])\n",
        "print(np.argmax(y_test[example]))\n",
        "prediction = model.predict(x_test[example:example+1])\n",
        "print(prediction)\n",
        "print(np.argmax(prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "6\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8dEnDiRbn7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0761cb9-366d-4ac6-f94b-ff4e8dbf1172"
      },
      "source": [
        "print(y_test_copy[example])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6yg4XFhbn7j"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkBmH69Hbn7m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "47367106-50c4-4894-95d1-2846caaff333"
      },
      "source": [
        "plt.imshow(x_train[example], cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7d2fddbd1870>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QoYZjGJ53SZ"
      },
      "source": [
        " ## Why such a low accuracy? Let's have a look at the input.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTJoZUa852SP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c733b1d-cfc1-48a2-f2ef-fee42e83ecc8"
      },
      "source": [
        "print(x_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
            "  175  26 166 255 247 127   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
            "  225 172 253 242 195  64   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
            "   93  82  82  56  39   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
            "   25   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
            "  150  27   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
            "  253 187   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
            "  253 249  64   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
            "  253 207   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
            "  250 182   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
            "   78   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do1WShssbn7q"
      },
      "source": [
        "## Feeding in normalized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5_fkAlNbn7q",
        "outputId": "f55ecbc6-8f95-4a02-fec8-8e9b8e95d185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "import keras\n",
        "# from tensorflow.python.keras.datasets import mnist\n",
        "# from tensorflow.python.keras import Sequential\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "\n",
        "\n",
        "\n",
        "#import keras\n",
        "#from keras.datasets import mnist\n",
        "#from keras.models import Sequential\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 20\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255                                # <<<<< notice this\n",
        "x_test /= 255                                 # <<<<< notice this\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "from keras.utils import np_utils\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_dim=784))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print('Test loss:', loss_and_metrics[0])\n",
        "print('Test accuracy:', loss_and_metrics[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-9263f0ce22f1>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# y_train = keras.utils.to_categorical(y_train, num_classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# y_test = keras.utils.to_categorical(y_test, num_classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'np_utils' from 'keras.utils' (/usr/local/lib/python3.10/dist-packages/keras/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GKELIv-bn7t"
      },
      "source": [
        "## Run this multiple times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4W1LdFKbn7u"
      },
      "source": [
        "import keras\n",
        "from tensorflow.python.keras.datasets import mnist\n",
        "from tensorflow.python.keras import Sequential\n",
        "#from keras.datasets import mnist\n",
        "#from keras.models import Sequential\n",
        "\n",
        "#import keras\n",
        "#from keras.datasets import mnist\n",
        "#from keras.models import Sequential\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 5\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "from keras.utils import np_utils\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_dim=784))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print('Test loss:', loss_and_metrics[0])\n",
        "print('Test accuracy:', loss_and_metrics[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXkl1E4Abn7x"
      },
      "source": [
        "## Initializers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_ooqtWhbn7y",
        "outputId": "dabb84fd-4c74-4896-e1ad-b4d11bcd3f40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "# from tensorflow.python.keras.datasets import mnist\n",
        "# from tensorflow.python.keras import Sequential\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "\n",
        "#import keras\n",
        "#from keras.datasets import mnist\n",
        "#from keras.models import Sequential\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 5\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "# from keras.utils import np_utils\n",
        "# y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "# y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, kernel_initializer='lecun_uniform', activation='relu', input_dim=784))\n",
        "model.add(Dense(512, kernel_initializer='lecun_uniform', activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print('Test loss:', loss_and_metrics[0])\n",
        "print('Test accuracy:', loss_and_metrics[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 3s 4ms/step - loss: nan - accuracy: 0.0989\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0987\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0987\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0987\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0987\n",
            "79/79 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0980\n",
            "Test loss: nan\n",
            "Test accuracy: 0.09799999743700027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wadGt6rXbn73"
      },
      "source": [
        "## Batch normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ra1DzS_bn74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373d5529-ca32-423e-a824-cadd003ddb4a"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "#from tensorflow.python.keras.datasets import mnist\n",
        "#from tensorflow.python.keras import Sequential\n",
        "#import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 5\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(Dense(512, activation='relu', input_dim=784))\n",
        "model.add(keras.layers.BatchNormalization())                   ### <<<<< notice this\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print('Test loss:', loss_and_metrics[0])\n",
        "print('Test accuracy:', loss_and_metrics[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 3s 4ms/step - loss: 0.4635 - accuracy: 0.8666\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2192 - accuracy: 0.9375\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1704 - accuracy: 0.9521\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1425 - accuracy: 0.9598\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1226 - accuracy: 0.9657\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.1253 - accuracy: 0.9618\n",
            "Test loss: 0.12525464594364166\n",
            "Test accuracy: 0.9617999792098999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZDp68YVbn79"
      },
      "source": [
        "### Combining things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOsnCfHGbn7-",
        "outputId": "8dd74ebe-9c39-41a5-d159-e7e4b01b6e45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# from keras.utils import np_utils\n",
        "from tensorflow.keras import layers\n",
        "#from tensorflow.python.keras.datasets import mnist\n",
        "#from tensorflow.python.keras import Sequential\n",
        "#import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "\n",
        "\n",
        "#import keras\n",
        "#from keras.datasets import mnist\n",
        "#from keras.models import Sequential\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 5\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "# from keras.utils import np_utils\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(Dense(512, kernel_initializer='he_normal', activation='relu', input_dim=784))\n",
        "model.add(keras.layers.BatchNormalization())                   ### <<<<< notice this\n",
        "model.add(Dense(512, kernel_initializer='he_normal', activation='relu'))\n",
        "model.add(Dense(512, kernel_initializer='he_normal', activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print('Test loss:', loss_and_metrics[0])\n",
        "print('Test accuracy:', loss_and_metrics[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 3s 4ms/step - loss: 0.4626 - accuracy: 0.8635\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.2076 - accuracy: 0.9398\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.1563 - accuracy: 0.9539\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1285 - accuracy: 0.9623\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1079 - accuracy: 0.9692\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.1227 - accuracy: 0.9619\n",
            "Test loss: 0.12269898504018784\n",
            "Test accuracy: 0.961899995803833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fnuLFeXnwOm"
      },
      "source": [
        "##The 2nd dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbPHc60ibn8B",
        "outputId": "1af40452-2c23-4b32-87af-a60ef3d0e599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 5\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.reshape(50000, 3072)\n",
        "x_test = x_test.reshape(10000, 3072)\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(512, activation='relu', input_dim=3072),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print('Test loss:', loss_and_metrics[0])\n",
        "print('Test accuracy:', loss_and_metrics[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.9869 - accuracy: 0.2930\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 1.8158 - accuracy: 0.3635\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 1.7434 - accuracy: 0.3910\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 1.6932 - accuracy: 0.4073\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6534 - accuracy: 0.4219\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 1.6839 - accuracy: 0.4093\n",
            "Test loss: 1.6839364767074585\n",
            "Test accuracy: 0.4092999994754791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHk5FRYy9tDZ"
      },
      "source": [
        "## Let's Play with a CNN now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MBSSgtG1_p_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb07595-57c6-4825-90fa-081345d5bf10"
      },
      "source": [
        "import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "# from keras.utils import np_utils\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# from keras import optimizers\n",
        "import numpy as np\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    elif epoch > 100:\n",
        "        lrate = 0.0003\n",
        "    return lrate\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#z-score\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)\n",
        "\n",
        "num_classes = 10\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "weight_decay = 1e-4\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "datagen.fit(x_train)\n",
        "\n",
        "#training\n",
        "batch_size = 64\n",
        "opt_rms = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "# opt_rms = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=2,\\\n",
        "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
        "\n",
        "\n",
        "#save to disk\n",
        "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "del model  # deletes the existing model\n",
        "\n",
        "# returns a compiled model\n",
        "# identical to the previous one\n",
        "model = load_model('my_model.h5')\n",
        "\n",
        "#testing\n",
        "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
        "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 32, 32, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 32, 32, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 16, 16, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 16, 16, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 16, 16, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 8, 8, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 8, 8, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 309290 (1.18 MB)\n",
            "Trainable params: 308394 (1.18 MB)\n",
            "Non-trainable params: 896 (3.50 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-fc9d3451b261>:90: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "781/781 [==============================] - 40s 41ms/step - loss: 2.0150 - accuracy: 0.3770 - val_loss: 1.4627 - val_accuracy: 0.5012 - lr: 0.0010\n",
            "Epoch 2/2\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 1.5121 - accuracy: 0.4912 - val_loss: 1.5448 - val_accuracy: 0.4988 - lr: 0.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 1s 9ms/step - loss: 1.5448 - accuracy: 0.4988\n",
            "\n",
            "Test result: 49.880 loss: 1.545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c30Jj34wVwgF"
      },
      "source": [
        "\n",
        "# from keras import optimizers\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n",
        "model.add(Activation('softmax'))\n",
        "sgd = optimizers.SGD(learning_rate=0.01,  momentum=0.9, nesterov=True)\n",
        "# sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='mean_squared_error', optimizer=sgd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P7YvSYI2AiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c9d7df-4411-4d6a-c215-4cd9609e888a"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "from keras.models import Sequential\n",
        "# from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#z-score\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)\n",
        "\n",
        "num_classes = 10\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "weight_decay = 1e-4\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "opt_rms = keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "# opt_rms = keras.optimizers.RMSprop(learning_rate=0.001, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=2, batch_size=32, verbose=1,validation_data=(x_test,y_test))\n",
        "\n",
        "#testing\n",
        "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
        "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 16, 16, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 8, 8, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPoolin  (None, 4, 4, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113738 (444.29 KB)\n",
            "Trainable params: 113738 (444.29 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "1563/1563 [==============================] - 10s 5ms/step - loss: 1.3780 - accuracy: 0.5191 - val_loss: 1.2542 - val_accuracy: 0.5764\n",
            "Epoch 2/2\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1358 - accuracy: 0.6177 - val_loss: 1.0797 - val_accuracy: 0.6298\n",
            "79/79 [==============================] - 0s 4ms/step - loss: 1.0797 - accuracy: 0.6298\n",
            "\n",
            "Test result: 62.980 loss: 1.080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi1GobkFl31c"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}